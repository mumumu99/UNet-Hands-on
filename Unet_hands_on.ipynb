{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Unet_hands_on.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1wksKQH841YVR7uxW2Z_pr65xijP819R7","authorship_tag":"ABX9TyO+IJiIheccaXWM2sd/jXoZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"xGen06RyWo6X"},"source":["import torch\n","import torch.nn as nn\n","import torchvision.transforms.functional as TF\n","\n","class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","    def forward(self, x):\n","        return self.conv(x)\n","\n","class UNET(nn.Module):\n","    def __init__(\n","            self, in_channels=3, out_channels=1, features=[64, 128, 256, 512],\n","    ):\n","        super(UNET, self).__init__()\n","        self.ups = nn.ModuleList()\n","        self.downs = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n","\n","        # Down part of UNET\n","        for feature in features:\n","            self.downs.append(DoubleConv(in_channels, feature))\n","            in_channels = feature\n","\n","        # Up part of UNET\n","        for feature in reversed(features):\n","            self.ups.append(\n","                nn.ConvTranspose2d(\n","                    feature*2, feature, kernel_size=2, stride=2,\n","                )\n","            )\n","            self.ups.append(DoubleConv(feature*2, feature))\n","\n","        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n","        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n","\n","    def forward(self, x):\n","        skip_connections = []\n","\n","        for down in self.downs:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)\n","        skip_connections = skip_connections[::-1]\n","\n","        for idx in range(0, len(self.ups), 2): \n","            x = self.ups[idx](x)\n","            skip_connection = skip_connections[idx//2] \n","\n","            if x.shape != skip_connection.shape:\n","                x = TF.resize(x, size=skip_connection.shape[2:])\n","\n","            concat_skip = torch.cat((skip_connection, x), dim=1)\n","            x = self.ups[idx+1](concat_skip) \n","\n","        return self.final_conv(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibaWVsiDccNg"},"source":["# google drive를 mount 시키기 (데이터셋 연동을 위함)\n","from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jHLEMfauiQju"},"source":["import os\n","os.chdir('/content/drive/My Drive/Colab Notebooks/UNet Hands-on')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CYnQ6CH2dMPq"},"source":["!pip install albumentations==0.4.6"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rwFO9CgiWXKx"},"source":["import torch\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","from utils import (\n","    load_checkpoint,\n","    save_checkpoint,\n","    get_loaders,\n","    check_accuracy,\n","    save_predictions_as_imgs,\n",")\n","\n","# Hyperparameters etc.\n","LEARNING_RATE = 1e-4\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","BATCH_SIZE = 16\n","NUM_EPOCHS = 3\n","NUM_WORKERS = 2\n","IMAGE_HEIGHT = 160  # 1280 originally\n","IMAGE_WIDTH = 240  # 1918 originally\n","PIN_MEMORY = True\n","LOAD_MODEL = True\n","TRAIN_IMG_DIR = \"data/train_images/\"\n","TRAIN_MASK_DIR = \"data/train_masks/\"\n","VAL_IMG_DIR = \"data/val_images/\"\n","VAL_MASK_DIR = \"data/val_masks/\"\n","\n","def train_fn(loader, model, optimizer, loss_fn, scaler):\n","    loop = tqdm(loader)\n","\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=DEVICE)\n","        targets = targets.float().unsqueeze(1).to(device=DEVICE)\n","\n","        # Forward\n","        with torch.cuda.amp.autocast():\n","            predictions = model(data)\n","            loss = loss_fn(predictions, targets)\n","\n","        # Backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # Update tqdm loop\n","        loop.set_postfix(loss=loss.item())\n","\n","def main():\n","    # Train 시 적용하는 augmentation \n","    train_transform = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Rotate(limit=35, p=1.0),\n","            A.HorizontalFlip(p=0.5),\n","            A.VerticalFlip(p=0.1),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0,\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","    # Validation 시 적용하는 augmentation.\n","    val_transforms = A.Compose(\n","        [\n","            A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","            A.Normalize(\n","                mean=[0.0, 0.0, 0.0],\n","                std=[1.0, 1.0, 1.0],\n","                max_pixel_value=255.0,\n","            ),\n","            ToTensorV2(),\n","        ],\n","    )\n","\n","    ## Model 및 loss function, optimizer 정의.\n","    model = UNET(in_channels=3, out_channels=1).to(DEVICE)\n","    loss_fn = nn.BCEWithLogitsLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","    ## Data loader 정의.\n","    train_loader, val_loader = get_loaders(\n","        TRAIN_IMG_DIR,\n","        TRAIN_MASK_DIR,\n","        VAL_IMG_DIR,\n","        VAL_MASK_DIR,\n","        BATCH_SIZE,\n","        train_transform,\n","        val_transforms,\n","        NUM_WORKERS,\n","        PIN_MEMORY,\n","    )\n","\n","    if LOAD_MODEL:\n","        load_checkpoint(torch.load(\"my_checkpoint_pretrained.pth.tar\"), model)\n","\n","    check_accuracy(val_loader, model, device=DEVICE)\n","    scaler = torch.cuda.amp.GradScaler()\n","\n","    for epoch in range(NUM_EPOCHS):\n","        train_fn(train_loader, model, optimizer, loss_fn, scaler)\n","\n","        # Save model\n","        checkpoint = {\n","            # Pytorch에서 모델의 state_dict은 학습가능한 매개변수 (weight & bias)가 담겨있는 딕셔너리(Dictionary)입니다. \n","            \"state_dict\": model.state_dict(),\n","            \"optimizer\":optimizer.state_dict(),\n","        }\n","        save_checkpoint(checkpoint)\n","\n","        # Check accuracy\n","        check_accuracy(val_loader, model, device=DEVICE)\n","\n","        # Print some examples to a folder\n","        save_predictions_as_imgs(\n","            val_loader, model, folder=\"saved_images/\", device=DEVICE\n","        )\n","\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":null,"outputs":[]}]}